<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Introduction to rags2ridges • rags2ridges</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/simplex/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Introduction to rags2ridges">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">rags2ridges</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.2.8</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../articles/rags2ridges.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/CFWP/rags2ridges/" class="external-link">
    <span class="fab fa-github fa-lg"></span>

  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><script src="rags2ridges_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Introduction to rags2ridges</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/CFWP/rags2ridges/blob/master/vignettes/rags2ridges.Rmd" class="external-link"><code>vignettes/rags2ridges.Rmd</code></a></small>
      <div class="hidden name"><code>rags2ridges.Rmd</code></div>

    </div>

    
    
<p><strong>rags2ridges</strong> is an R-package for <em>fast</em> and <em>proper</em> L2-penalized estimation of precision (and covariance) matrices also called <strong>ridge estimation</strong>. Its L2-penalty features the ability to shrink towards a target matrix, allowing for incorporation of prior knowledge. Likewise, it also features a <em>fused</em> L2 ridge penalty allows for simultaneous estimation of multiple matrices. The package also contains additional functions for post-processing the L2-penalized estimates — useful for feature selection and when doing graphical modelling. The <em>fused</em> ridge estimation is useful when dealing with grouped data as when doing meta or integrative analysis.</p>
<p>This vignette provides a light introduction on how to get started with regular ridge estimation of precision matrices and further steps.</p>
<div class="section level2">
<h2 id="getting-started">Getting started<a class="anchor" aria-label="anchor" href="#getting-started"></a>
</h2>
<div class="section level3">
<h3 id="package-installation">Package installation<a class="anchor" aria-label="anchor" href="#package-installation"></a>
</h3>
<p>The README details how to install the <strong>rags2ridges</strong> package. When installed, the package is loaded as seen below where we also define a function for adding pretty names to a matrix.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://cfwp.github.io/rags2ridges/">rags2ridges</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="small-theoretical-primer-and-package-usage">Small theoretical primer and package usage<a class="anchor" aria-label="anchor" href="#small-theoretical-primer-and-package-usage"></a>
</h3>
<p>The sample variance-covariance matrix, or simply <em>covariance matrix</em>, is well-known and ubiquitous. It is given by</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mi>X</mi><msup><mi>X</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">
S = \frac{1}{n - 1}XX^T
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n \times p</annotation></semantics></math> data matrix that is zero-centered with each <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-dimensional observations in the rows. I.e. each row of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is an observation and each column is feature. Often high-dimensional data is organised this way (or transposed).</p>
<p>That <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is zero-centered simply means that the column means has been subtracted the columns. The very similar estimate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mi>X</mi><msup><mi>X</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">S = \frac{1}{n}XX^T</annotation></semantics></math> without <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction" class="external-link">Bessel’s correction</a> is the maximum likelihood estimate in a multivariate normal model with mean <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math> and covariance <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>. The likelihood function in this case is given by</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℓ</mi><mo stretchy="false" form="prefix">(</mo><mi>Ω</mi><mo>;</mo><mi>S</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo>ln</mo><mo stretchy="false" form="prefix">|</mo><mi>Ω</mi><mo stretchy="false" form="prefix">|</mo><mo>−</mo><mtext mathvariant="normal">tr</mtext><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mi>Ω</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
\ell(\Omega; S) = \ln|\Omega| - \text{tr}(S\Omega)
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ω</mi><mo>=</mo><msup><mi>Σ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\Omega = \Sigma^{-1}</annotation></semantics></math> is the so-called precision matrix (also sometimes called the <em>concentration matrix</em>). It is precisely this <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math> for which we seek an estimate we will denote <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>. Indeed, one can naturally try to use the inverse of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> for this:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>=</mo><msup><mi>S</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">
P = S^{-1}
</annotation></semantics></math></p>
<p>Let’s try.</p>
<p>The <code><a href="../reference/createS.html">createS()</a></code> function can easily simulate covariance matrices. But we go a more verbose route for illustration:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">6</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">20</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/createS.html">createS</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, p <span class="op">=</span> <span class="va">p</span>, dataset <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">X</span>, n <span class="op">=</span> <span class="fl">4</span><span class="op">)</span> <span class="co"># Show 4 first of the n rows</span></span></code></pre></div>
<pre><code><span><span class="co">##            A        B      C      D      E      F</span></span>
<span><span class="co">## [1,]  0.9714 -0.00289  0.255  0.168  0.912  1.619</span></span>
<span><span class="co">## [2,] -1.0060  0.41324 -0.553  1.397  0.205 -1.851</span></span>
<span><span class="co">## [3,] -0.0843  0.72443  1.405 -0.679  2.584  1.055</span></span>
<span><span class="co">## [4,] -0.5541  2.35394 -0.795  0.738 -0.789 -0.805</span></span></code></pre>
<p>Here the columns corresponds to features A, B, C, and so on.</p>
<p>When can then arrive a the MLE using <code><a href="../reference/covML.html">covML()</a></code> which <em>centers</em> X (subtracting the column means) and then computes the estimate:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">S</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/covML.html">covML</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">S</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##          A       B        C        D      E       F</span></span>
<span><span class="co">## A  0.82223 -0.1797  0.09785 -0.00908 0.0601  0.1334</span></span>
<span><span class="co">## B -0.17972  0.6811 -0.06728  0.08899 0.2782 -0.0504</span></span>
<span><span class="co">## C  0.09785 -0.0673  0.71338 -0.00948 0.2556 -0.0970</span></span>
<span><span class="co">## D -0.00908  0.0890 -0.00948  1.23448 0.1960 -0.3400</span></span>
<span><span class="co">## E  0.06014  0.2782  0.25556  0.19603 1.7084  0.2202</span></span>
<span><span class="co">## F  0.13338 -0.0504 -0.09701 -0.34005 0.2202  1.0783</span></span></code></pre>
<p>Using <code><a href="https://rdrr.io/r/stats/cor.html" class="external-link">cov2cor()</a></code> the well-known correlation matrix could be obtained.</p>
<p>By default, <code><a href="../reference/createS.html">createS()</a></code> simulates zero-mean i.i.d. normal variables (corresponding to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Σ</mi><mo>=</mo><mi>Ω</mi><mo>=</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">\Sigma=\Omega=I</annotation></semantics></math> being the identity matrix), but it has plenty of possibilities for more intricate covariance structures. The <code>S</code> matrix could have been obtained directly had we omitted the <code>dataset</code> argument, leaving it to be the default <code>FALSE</code>. The <code><a href="../reference/rmvnormal.html">rmvnormal()</a></code> function is utilized by <code><a href="../reference/createS.html">createS()</a></code> to generate the normal sample.</p>
<p>We can obtain the precision estimate <code>P</code> using <code><a href="https://rdrr.io/r/base/solve.html" class="external-link">solve()</a></code> to invert <code>S</code>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">P</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html" class="external-link">solve</a></span><span class="op">(</span><span class="va">S</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">P</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##         A       B      C       D       E      F</span></span>
<span><span class="co">## A  1.3433  0.3557 -0.155 -0.0552 -0.0537 -0.170</span></span>
<span><span class="co">## B  0.3557  1.7418  0.255 -0.0327 -0.3462  0.121</span></span>
<span><span class="co">## C -0.1553  0.2555  1.604  0.1218 -0.3262  0.280</span></span>
<span><span class="co">## D -0.0552 -0.0327  0.122  0.9334 -0.1624  0.344</span></span>
<span><span class="co">## E -0.0537 -0.3462 -0.326 -0.1624  0.7422 -0.242</span></span>
<span><span class="co">## F -0.1700  0.1208  0.280  0.3438 -0.2417  1.137</span></span></code></pre>
<p>That’s it! Everything goes well here only because <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&lt;</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n &lt; p</annotation></semantics></math>. However, when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> is close to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>, the estimate become unstable and varies wildly and when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> exceeds <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> one can no longer invert <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> and this strategy fails:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">25</span></span>
<span><span class="va">S2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/createS.html">createS</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, p <span class="op">=</span> <span class="va">p</span><span class="op">)</span>  <span class="co"># Direct to S</span></span>
<span><span class="va">P2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html" class="external-link">solve</a></span><span class="op">(</span><span class="va">S2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Error in solve.default(S2): system is computationally singular: reciprocal condition number = 2.00171e-18</span></span></code></pre>
<p>Note that this is now a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn><mo>×</mo><mn>25</mn></mrow><annotation encoding="application/x-tex">25 \times 25</annotation></semantics></math> precision matrix we are trying to estimate. Datasets where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">p &gt; n</annotation></semantics></math> are starting to be common, so what now?</p>
<p>To solve the problem, <strong>rags2ridges</strong> adds a so-called ridge penalty to the likelihood above — this method is also called <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_2</annotation></semantics></math> shrinkage and works by “shrinking” the eigenvalues of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> in a particular manner to combat that they “explode” when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>≥</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">p \geq n</annotation></semantics></math>.</p>
<p>The core problem that <strong>rags2ridges</strong> solves is that</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℓ</mi><mo stretchy="false" form="prefix">(</mo><mi>Ω</mi><mo>;</mo><mi>S</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo>ln</mo><mo stretchy="false" form="prefix">|</mo><mi>Ω</mi><mo stretchy="false" form="prefix">|</mo><mo>−</mo><mtext mathvariant="normal">tr</mtext><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mi>Ω</mi><mo stretchy="false" form="postfix">)</mo><mo>−</mo><mfrac><mi>λ</mi><mn>2</mn></mfrac><mo stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><mi>Ω</mi><mo>−</mo><mi>T</mi><mo stretchy="false" form="prefix">|</mo><msubsup><mo stretchy="false" form="prefix">|</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">
\ell(\Omega; S) = \ln|\Omega| - \text{tr}(S\Omega) - \frac{\lambda}{2}|| \Omega - T||^2_2
</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda &gt; 0</annotation></semantics></math> is the ridge penalty parameter, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">p \times p</annotation></semantics></math> known <em>target</em> matrix (which we will get back to) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><mo>⋅</mo><mo stretchy="false" form="prefix">|</mo><msub><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">||\cdot||_2</annotation></semantics></math> is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_2</annotation></semantics></math>-norm. The maximizing solution here is surprisingly on closed form, but it is rather complicated<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Assume for now the target matrix is an all zero matrix and thus out of the equation.</p>
<p>The core function of <strong>rags2ridges</strong> is <code>ridgeP</code> which computes this estimate in a fast manner.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">P2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ridgeP.html">ridgeP</a></span><span class="op">(</span><span class="va">S2</span>, lambda <span class="op">=</span> <span class="fl">1.17</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">P2</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">7</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">7</span><span class="op">]</span><span class="op">)</span> <span class="co"># Showing only the 7 first cols and rows</span></span></code></pre></div>
<pre><code><span><span class="co">##         A       B       C       D       E       F      G</span></span>
<span><span class="co">## A  3.1593 -0.3941  0.1241  0.2530  0.0858 -0.1183  0.166</span></span>
<span><span class="co">## B -0.3941  3.1671 -0.0414  0.1595  0.0402 -0.1484 -0.303</span></span>
<span><span class="co">## C  0.1241 -0.0414  3.3744 -0.3119  0.2454  0.2537  0.175</span></span>
<span><span class="co">## D  0.2530  0.1595 -0.3119  2.4216  0.0813  0.0798  0.411</span></span>
<span><span class="co">## E  0.0858  0.0402  0.2454  0.0813  3.4259 -0.2761 -0.052</span></span>
<span><span class="co">## F -0.1183 -0.1484  0.2537  0.0798 -0.2761  3.6691 -0.151</span></span>
<span><span class="co">## G  0.1664 -0.3028  0.1746  0.4112 -0.0520 -0.1510  3.019</span></span></code></pre>
<p>And voilà, we have our estimate. We will now discuss the penalty parameters and target matrix and how to choose them.</p>
</div>
<div class="section level3">
<h3 id="the-penalty-parameter">The penalty parameter<a class="anchor" aria-label="anchor" href="#the-penalty-parameter"></a>
</h3>
<p>The penalty parameter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> (<code>lambda</code>) shrinks the values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> such toward 0 (when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">T = 0</annotation></semantics></math>) — i.e. very larges values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> makes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> “small” and more stable whereas smaller values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> makes the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> tend toward the (possibly non-existent) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>S</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">S^{-1}</annotation></semantics></math>. So what <code>lambda</code> should you choose? One strategy for choosing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> is selecting it to be stable yet precise (a bias-variance trade-off). Automatic k-fold cross-validation can be done with <code><a href="../reference/optPenalty.kCVauto.html">optPenalty.kCVauto()</a></code>is well suited for this:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/createS.html">createS</a></span><span class="op">(</span><span class="va">n</span>, <span class="va">p</span>, dataset <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">opt</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/optPenalty.kCVauto.html">optPenalty.kCVauto</a></span><span class="op">(</span><span class="va">Y</span>, lambdaMin <span class="op">=</span> <span class="fl">0.001</span>, lambdaMax <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">opt</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 2</span></span>
<span><span class="co">##  $ optLambda: num 1.25</span></span>
<span><span class="co">##  $ optPrec  : 'ridgeP' num [1:25, 1:25] 2.7038 -0.0688 -0.1186 -0.0218 -0.1712 ...</span></span>
<span><span class="co">##   ..- attr(*, "lambda")= num 1.25</span></span>
<span><span class="co">##   ..- attr(*, "dimnames")=List of 2</span></span>
<span><span class="co">##   .. ..$ : chr [1:25] "A" "B" "C" "D" ...</span></span>
<span><span class="co">##   .. ..$ : chr [1:25] "A" "B" "C" "D" ...</span></span></code></pre>
<p>As seen, the function returns a list with the optimal penalty parameter and corresponding ridge precision estimate. By default, the the functions performs leave-one-out cross validation. See ?optPenalty.kCVauto` for more information.</p>
</div>
<div class="section level3">
<h3 id="the-target-matrix">The target matrix<a class="anchor" aria-label="anchor" href="#the-target-matrix"></a>
</h3>
<p>The target matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is a matrix the same size as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> which the estimate is “shrunken” toward — i.e. for large values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> the estimate goes toward <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>. The choice of the target is another subject. While one might first think that the all-zeros <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">T = [0]</annotation></semantics></math> would be a default it is intuitively not a good target. This is because we’d like an estimate that is positive definite (the matrix-equivalent to at positive number) and the null-matrix is not positive definite.</p>
<p>If one has a very good prior estimate or some other information this might used to construct the target. E.g. the function <code><a href="../reference/kegg.target.html">kegg.target()</a></code> utilizes the <em>Kyoto Encyclopedia of Genes and Genomes</em> (KEGG) database of gene and gene-networks together with pilot data to construct a target.</p>
<p>In the absence of such knowledge, the default could be a data-driven diagonal matrix. The function <code><a href="../reference/default.target.html">default.target()</a></code> offers some different approaches to selecting this. A good choice here is often the diagonal matrix times the reciprocal mean of the eigenvalues of the sample covariance as entries. See <code><a href="../reference/default.target.html">?default.target</a></code> for more choices.</p>
</div>
<div class="section level3">
<h3 id="gaussian-graphical-modeling-and-post-processing">Gaussian graphical modeling and post processing<a class="anchor" aria-label="anchor" href="#gaussian-graphical-modeling-and-post-processing"></a>
</h3>
<blockquote>
<h4 id="what-is-so-interesting-with-the-precision-matrix-anyway-im-always-interested-in-correlations-and-thus-the-correlation-matrix.">What is so interesting with the precision matrix anyway? I’m always interested in correlations and thus the correlation matrix.</h4>
</blockquote>
<p>As you may know, correlation does not imply causation. Nor does covariance imply causation. However, precision matrix provides stronger hints at causation. A relatively simple transformation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> maps it to partial correlations—much like how the sample covariance <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> easily maps to the correlation matrix. More precisely, the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>j</mi></mrow><annotation encoding="application/x-tex">ij</annotation></semantics></math>th partial correlation is given by</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ρ</mi><mrow><mi>i</mi><mi>j</mi><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="normal">all others</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mo>−</mo><msub><mi>p</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><msqrt><mrow><msub><mi>p</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><msub><mi>p</mi><mrow><mi>j</mi><mi>j</mi></mrow></msub></mrow></msqrt></mfrac></mrow><annotation encoding="application/x-tex">
\rho_{ij|\text{all others}} = \frac{- p_{ij}}{\sqrt{p_{ii}p_{jj}}}
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">p_{ij}</annotation></semantics></math> is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>j</mi></mrow><annotation encoding="application/x-tex">ij</annotation></semantics></math>th entry of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>.</p>
<p>Partial correlations measure the linear association between two random variables whilst removing the effect of other random variables; in this case, it is all the remaining variables. This somewhat absolves the issue in “regular” correlations where are often correlated but only indirectly; either by sort of ‘transitivity of correlations’ (which does not hold generally and is not<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> so<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> simple<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>) or by common underlying variables.</p>
<blockquote>
<h4 id="ok-but-what-is-graphical-about-the-graphical-ridge-estimate">OK, but what is <strong>graphical</strong> about the graphical ridge estimate?</h4>
</blockquote>
<p>In a multivariate normal model, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>p</mi><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">p_{ij} = p_{ji} = 0</annotation></semantics></math> if and only if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_i</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>j</mi></msub><annotation encoding="application/x-tex">X_j</annotation></semantics></math> are conditionally independent when condition on all other variables. I.e. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_i</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>j</mi></msub><annotation encoding="application/x-tex">X_j</annotation></semantics></math> are conditionally independent given all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>k</mi></msub><annotation encoding="application/x-tex">X_k</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>≠</mo><mi>i</mi></mrow><annotation encoding="application/x-tex">k \neq i</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>≠</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">k \neq j</annotation></semantics></math> if and when the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>j</mi></mrow><annotation encoding="application/x-tex">ij</annotation></semantics></math>th and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">{ji}</annotation></semantics></math>th elements of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> are zero. In real world applications, this means that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> is often relatively sparse (lots of zeros). This also points to the close relationship between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> and the partial correlations.</p>
<p>The non-zero entries of the a symmetric PD matrix can them be interpreted the edges of a graph where nodes correspond to the variables.</p>
<blockquote>
<h4 id="graphical-ridge-estimation-why-not-graphical-lasso">Graphical ridge estimation? Why not graphical Lasso?</h4>
</blockquote>
<p>The graphical lasso (gLasso) is the L1-equivalent to graphical ridge. A nice feature of the L1 penalty automatically induces sparsity and thus also select the edges in the underlying graph. The L2 penalty of <em>rags2ridges</em> relies on an extra step that selects the edges after <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> is estimated. While some may argue this as a drawback (typically due to a lack of perceived simplicity), it is often beneficial to separate the “variable selection” and estimation.</p>
<p>First, a separate post-hoc selection step allows for greater flexibility.</p>
<p>Secondly, when co-linearity is present the L1 penalty is “unstable” in the selection between the items. I.e. if 2 covariances are co-linear only one of them will typically be selected in a unpredictable way whereas the L2 will put equal weight on both and “average” their effect. Ultimately, this means that the L2 estimate is typically more stable than the L1.</p>
<p>At last point to mention here is also that the true underlying graph might not always be very sparse (or sparse at all).</p>
<blockquote>
<h4 id="how-do-i-select-the-edges-then">How do I select the edges then?</h4>
</blockquote>
<p>The <code><a href="../reference/sparsify.html">sparsify()</a></code> functions lets you select the non-zero entries of P corresponding to edges. It supports a handful different approaches ranging from simple thresholding to false discovery rate based selection.</p>
<p>After edge select <code><a href="../reference/GGMnetworkStats.html">GGMnetworkStats()</a></code> can be utilized to get summary statistics of the resulting graph topology.</p>
</div>
</div>
<div class="section level2">
<h2 id="concluding-remarks">Concluding remarks<a class="anchor" aria-label="anchor" href="#concluding-remarks"></a>
</h2>
<p>The <code><a href="../reference/fullMontyS.html">fullMontyS()</a></code> function is a convenience wrapper getting from the data through the penalized estimate to the corresponding conditional independence graph and topology summaries.</p>
<p>For a full introduction to the theoretical properties as well as more context to the problem, see <a href="https://doi.org/10.1016/j.csda.2016.05.012" class="external-link">van Wieringen &amp; Peeters (2016)</a>.</p>
<p><strong>rags2ridges</strong> also comes with functionality for <em>targeted</em> and <em>grouped</em> (or, <em>fused</em>) graphical ridge regression called the fused graphical ridge. See <a href="https://arxiv.org/abs/1509.07982" class="external-link">[2]</a> below. The functions in this <code>rags2ridges</code> module are generally post-fixed with <code>.fused</code>.</p>
<div class="section level3">
<h3 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h3>
<p><a href="https://doi.org/10.1016/j.csda.2016.05.012" class="external-link">1.</a>. van Wieringen, W.N. and Peeters, C.F.W. <strong>(2016)</strong>. <a href="https://doi.org/10.1016/j.csda.2016.05.012" class="external-link"><em>Ridge Estimation of Inverse Covariance Matrices from High-Dimensional Data.</em></a> Computational Statistics &amp; Data Analysis, vol. 103: 284-303.</p>
<p><a href="https://arxiv.org/abs/1509.07982" class="external-link">2.</a> Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and van Wieringen, W.N. <strong>(2015)</strong>. <a href="https://arxiv.org/abs/1509.07982" class="external-link"><em>Targeted Fused Ridge Estimation of Inverse Covariance Matrices from Multiple High-Dimensional Data Classes.</em></a> arXiv:1509.07982 [stat.ME].</p>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Solution for the graphical ridge problem: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>λ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo minsize="3.0" maxsize="3.0" stretchy="false" form="prefix">{</mo><mo minsize="2.4" maxsize="2.4" stretchy="false" form="prefix">[</mo><mi>λ</mi><msub><mi>I</mi><mrow><mi>p</mi><mo>×</mo><mi>p</mi></mrow></msub><mo>+</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>−</mo><mi>λ</mi><mi>T</mi><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup><msup><mo minsize="2.4" maxsize="2.4" stretchy="false" form="postfix">]</mo><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>−</mo><mi>λ</mi><mi>T</mi><mo stretchy="false" form="postfix">)</mo><msup><mo minsize="3.0" maxsize="3.0" stretchy="false" form="postfix">}</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">
P(\lambda) 
= \Bigg\{ \bigg[ \lambda I_{p\times p} + \frac{1}{4}(S - \lambda T)^{2} \bigg]^{1/2} + \frac{1}{2}(S -\lambda T) \Bigg\}^{-1}
</annotation></semantics></math><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="https://stats.stackexchange.com/questions/181376/is-correlation-transitive" class="external-link uri">https://stats.stackexchange.com/questions/181376/is-correlation-transitive</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><a href="https://emilkirkegaard.dk/en/2016/02/causality-transitivity-and-correlation/" class="external-link uri">https://emilkirkegaard.dk/en/2016/02/causality-transitivity-and-correlation/</a><a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/" class="external-link uri">https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/</a><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Carel F.W. Peeters, Anders Ellern Bilgrau, Wessel N. van Wieringen.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

      </footer>
</div>






  </body>
</html>
