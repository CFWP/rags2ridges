<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Introduction to rags2ridges • rags2ridges</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/simplex/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Introduction to rags2ridges">
<meta property="og:description" content="rags2ridges">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">rags2ridges</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">2.2.5</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/rags2ridges.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/CFWP/rags2ridges/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="rags2ridges_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Introduction to rags2ridges</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/CFWP/rags2ridges/blob/master/vignettes/rags2ridges.Rmd"><code>vignettes/rags2ridges.Rmd</code></a></small>
      <div class="hidden name"><code>rags2ridges.Rmd</code></div>

    </div>

    
    
<p><strong>rags2ridges</strong> is an R-package for <em>fast</em> and <em>proper</em> L2-penalized estimation of precision (and covariance) matrices also called <strong>ridge estimation</strong>. Its L2-penalty features the ability to shrink towards a target matrix, allowing for incorporation of prior knowledge. Likewise, it also features a <em>fused</em> L2 ridge penalty allows for simultaneous estimation of multiple matrices. The package also contains additional functions for post-processing the L2-penalized estimates — useful for feature selection and when doing graphical modelling. The <em>fused</em> ridge estimation is useful when dealing with grouped data as when doing meta or integrative analysis.</p>
<p>This vignette provides a light introduction on how to get started with regular ridge estimation of precision matrices and further steps.</p>
<div id="getting-started" class="section level2">
<h2 class="hasAnchor">
<a href="#getting-started" class="anchor"></a>Getting started</h2>
<div id="package-installation" class="section level3">
<h3 class="hasAnchor">
<a href="#package-installation" class="anchor"></a>Package installation</h3>
<p>The README details how to install the <strong>rags2ridges</strong> package. When installed, the package is loaded as seen below where we also define a function for adding pretty names to a matrix.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://cfwp.github.io/rags2ridges">rags2ridges</a></span><span class="op">)</span></code></pre></div>
</div>
<div id="small-theoretical-primer-and-package-usage" class="section level3">
<h3 class="hasAnchor">
<a href="#small-theoretical-primer-and-package-usage" class="anchor"></a>Small theoretical primer and package usage</h3>
<p>The sample variance-covariance matrix, or simply <em>covariance matrix</em>, is well-known and ubiquitous. It is given by</p>
<p><span class="math display">\[
S = \frac{1}{n - 1}XX^T
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is the <span class="math inline">\(n \times p\)</span> data matrix that is zero-centered with each <span class="math inline">\(p\)</span>-dimensional observations in the rows. I.e. each row of <span class="math inline">\(X\)</span> is an observation and each column is feature. Often high-dimensional data is organised this way (or transposed).</p>
<p>That <span class="math inline">\(X\)</span> is zero-centered simply means that the column means has been subtracted the columns. The very similar estimate <span class="math inline">\(S = \frac{1}{n}XX^T\)</span> without <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a> is the maximum likelihood estimate in a multivariate normal model with mean <span class="math inline">\(0\)</span> and covariance <span class="math inline">\(\Sigma\)</span>. The likelihood function in this case is given by</p>
<p><span class="math display">\[
\ell(\Omega; S) = \ln|\Omega| - \text{tr}(S\Omega)
\]</span></p>
<p>where <span class="math inline">\(\Omega = \Sigma^{-1}\)</span> is the so-called precision matrix (also sometimes called the <em>concentration matrix</em>). It is precisely this <span class="math inline">\(\Omega\)</span> for which we seek an estimate we will denote <span class="math inline">\(P\)</span>. Indeed, one can naturally try to use the inverse of <span class="math inline">\(S\)</span> for this:</p>
<p><span class="math display">\[
P = S^{-1}
\]</span></p>
<p>Let’s try.</p>
<p>The <code><a href="../reference/createS.html">createS()</a></code> function can easily simulate covariance matrices. But we go a more verbose route for illustration:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">6</span>
<span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">20</span>
<span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/createS.html">createS</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, p <span class="op">=</span> <span class="va">p</span>, dataset <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">X</span>, n <span class="op">=</span> <span class="fl">4</span><span class="op">)</span> <span class="co"># Show 4 first of the n rows</span></code></pre></div>
<pre><code>##          A       B      C      D       E      F
## [1,] 0.521 -0.2866  0.304  0.576 -0.2545  0.562
## [2,] 0.380 -1.5223 -1.355  0.214 -1.1543  1.041
## [3,] 0.950  0.0394  0.583 -0.727 -0.5191  0.448
## [4,] 1.648 -0.7481  0.238 -2.175 -0.0498 -0.188</code></pre>
<p>Here the columns corresponds to features A, B, C, and so on.</p>
<p>When can then arrive a the MLE using <code><a href="../reference/covML.html">covML()</a></code> which <em>centers</em> X (subtracting the column means) and then computes the estimate:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">S</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/covML.html">covML</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">S</span><span class="op">)</span></code></pre></div>
<pre><code>##        A      B       C      D       E       F
## A  0.680 -0.206  0.1797  0.103  0.2141 -0.1656
## B -0.206  1.318 -0.5821 -0.256  0.1402 -0.0420
## C  0.180 -0.582  1.4715 -0.142 -0.0270 -0.0925
## D  0.103 -0.256 -0.1415  1.061  0.2026 -0.1278
## E  0.214  0.140 -0.0270  0.203  0.7237  0.0514
## F -0.166 -0.042 -0.0925 -0.128  0.0514  0.7978</code></pre>
<p>Using <code><a href="https://rdrr.io/r/stats/cor.html">cov2cor()</a></code> the well-known correlation matrix could be obtained.</p>
<p>By default, <code><a href="../reference/createS.html">createS()</a></code> simulates zero-mean i.i.d. normal variables (corresponding to <span class="math inline">\(\Sigma=\Omega=I\)</span> being the identity matrix), but it has plenty of possibilities for more intricate covariance structures. The <code>S</code> matrix could have been obtained directly had we omitted the <code>dataset</code> argument, leaving it to be the default <code>FALSE</code>. The <code><a href="../reference/rmvnormal.html">rmvnormal()</a></code> function is utilized by <code><a href="../reference/createS.html">createS()</a></code> to generate the normal sample.</p>
<p>We can obtain the precision estimate <code>P</code> using <code><a href="https://rdrr.io/r/base/solve.html">solve()</a></code> to invert <code>S</code>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">P</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">S</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">P</span><span class="op">)</span></code></pre></div>
<pre><code>##         A      B       C       D      E      F
## A  1.9271  0.384 -0.0582  0.0887 -0.705  0.473
## B  0.3835  1.168  0.4665  0.4308 -0.464  0.294
## C -0.0582  0.466  0.9070  0.2851 -0.131  0.172
## D  0.0887  0.431  0.2851  1.1980 -0.455  0.295
## E -0.7051 -0.464 -0.1314 -0.4555  1.830 -0.377
## F  0.4730  0.294  0.1718  0.2954 -0.377  1.459</code></pre>
<p>That’s it! Everything goes well here only because <span class="math inline">\(n &lt; p\)</span>. However, when <span class="math inline">\(p\)</span> is close to <span class="math inline">\(n\)</span>, the estimate become unstable and varies wildly and when <span class="math inline">\(p\)</span> exceeds <span class="math inline">\(n\)</span> one can no longer invert <span class="math inline">\(S\)</span> and this strategy fails:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">25</span>
<span class="va">S2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/createS.html">createS</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, p <span class="op">=</span> <span class="va">p</span><span class="op">)</span>  <span class="co"># Direct to S</span>
<span class="va">P2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">S2</span><span class="op">)</span></code></pre></div>
<pre><code>## Error in solve.default(S2): system is computationally singular: reciprocal condition number = 1.02021e-18</code></pre>
<p>Note that this is now a <span class="math inline">\(25 \times 25\)</span> precision matrix we are trying to estimate. Datasets where <span class="math inline">\(p &gt; n\)</span> are starting to be common, so what now?</p>
<p>To solve the problem, <strong>rags2ridges</strong> adds a so-called ridge penalty to the likelihood above — this method is also called <span class="math inline">\(L_2\)</span> shrinkage and works by “shrinking” the eigenvalues of <span class="math inline">\(S\)</span> in a particular manner to combat that they “explode” when <span class="math inline">\(p \geq n\)</span>.</p>
<p>The core problem that <strong>rags2ridges</strong> solves is that</p>
<p><span class="math display">\[
\ell(\Omega; S) = \ln|\Omega| - \text{tr}(S\Omega) - \frac{\lambda}{2}|| \Omega - T||^2_2
\]</span> where <span class="math inline">\(\lambda &gt; 0\)</span> is the ridge penalty parameter, <span class="math inline">\(T\)</span> is a <span class="math inline">\(p \times p\)</span> known <em>target</em> matrix (which we will get back to) and <span class="math inline">\(||\cdot||_2\)</span> is the <span class="math inline">\(L_2\)</span>-norm. The maximizing solution here is surprisingly on closed form, but it is rather complicated<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Assume for now the target matrix is an all zero matrix and thus out of the equation.</p>
<p>The core function of <strong>rags2ridges</strong> is <code>ridgeP</code> which computes this estimate in a fast manner.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">P2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ridgeP.html">ridgeP</a></span><span class="op">(</span><span class="va">S2</span>, lambda <span class="op">=</span> <span class="fl">1.17</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">P2</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">7</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">7</span><span class="op">]</span><span class="op">)</span> <span class="co"># Showing only the 7 first cols and rows</span></code></pre></div>
<pre><code>##         A        B       C       D       E        F       G
## A  2.3457  0.05383  0.1204  0.1274 -0.2327 -0.27675  0.1101
## B  0.0538  2.85155  0.2581 -0.0347 -0.0400  0.00857 -0.0177
## C  0.1204  0.25812  2.1084  0.2851 -0.0185  0.10897  0.3452
## D  0.1274 -0.03472  0.2851  2.1796  0.2019 -0.08931 -0.0660
## E -0.2327 -0.04000 -0.0185  0.2019  2.4352 -0.08725  0.1088
## F -0.2767  0.00857  0.1090 -0.0893 -0.0872  2.71035 -0.0284
## G  0.1101 -0.01771  0.3452 -0.0660  0.1088 -0.02841  2.8171</code></pre>
<p>And voilà, we have our estimate. We will now discuss the penalty parameters and target matrix and how to choose them.</p>
</div>
<div id="the-penalty-parameter" class="section level3">
<h3 class="hasAnchor">
<a href="#the-penalty-parameter" class="anchor"></a>The penalty parameter</h3>
<p>The penalty parameter <span class="math inline">\(\lambda\)</span> (<code>lambda</code>) shrinks the values of <span class="math inline">\(P\)</span> such toward 0 (when <span class="math inline">\(T = 0\)</span>) — i.e. very larges values of <span class="math inline">\(\lambda\)</span> makes <span class="math inline">\(P\)</span> “small” and more stable whereas smaller values of <span class="math inline">\(\lambda\)</span> makes the <span class="math inline">\(P\)</span> tend toward the (possibly non-existent) <span class="math inline">\(S^{-1}\)</span>. So what <code>lambda</code> should you choose? One strategy for choosing <span class="math inline">\(\lambda\)</span> is selecting it to be stable yet precise (a bias-variance trade-off). Automatic k-fold cross-validation can be done with <code><a href="../reference/optPenalty.kCVauto.html">optPenalty.kCVauto()</a></code>is well suited for this:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/createS.html">createS</a></span><span class="op">(</span><span class="va">n</span>, <span class="va">p</span>, dataset <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="va">opt</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/optPenalty.kCVauto.html">optPenalty.kCVauto</a></span><span class="op">(</span><span class="va">Y</span>, lambdaMin <span class="op">=</span> <span class="fl">0.001</span>, lambdaMax <span class="op">=</span> <span class="fl">100</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">opt</span><span class="op">)</span></code></pre></div>
<pre><code>## List of 2
##  $ optLambda: num 0.743
##  $ optPrec  : 'ridgeP' num [1:25, 1:25] 2.8329 -0.0927 0.123 -0.105 0.3097 ...
##   ..- attr(*, "lambda")= num 0.743
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:25] "A" "B" "C" "D" ...
##   .. ..$ : chr [1:25] "A" "B" "C" "D" ...</code></pre>
<p>As seen, the function returns a list with the optimal penalty parameter and corresponding ridge precision estimate. By default, the the functions performs leave-one-out cross validation. See ?optPenalty.kCVauto` for more information.</p>
</div>
<div id="the-target-matrix" class="section level3">
<h3 class="hasAnchor">
<a href="#the-target-matrix" class="anchor"></a>The target matrix</h3>
<p>The target matrix <span class="math inline">\(T\)</span> is a matrix the same size as <span class="math inline">\(P\)</span> which the estimate is “shrunken” toward — i.e. for large values of <span class="math inline">\(\lambda\)</span> the estimate goes toward <span class="math inline">\(T\)</span>. The choice of the target is another subject. While one might first think that the all-zeros <span class="math inline">\(T = [0]\)</span> would be a default it is intuitively not a good target. This is because we’d like an estimate that is positive definite (the matrix-equivalent to at positive number) and the null-matrix is not positive definite.</p>
<p>If one has a very good prior estimate or some other information this might used to construct the target. E.g. the function <code><a href="../reference/kegg.target.html">kegg.target()</a></code> utilizes the <em>Kyoto Encyclopedia of Genes and Genomes</em> (KEGG) database of gene and gene-networks together with pilot data to construct a target.</p>
<p>In the absence of such knowledge, the default could be a data-driven diagonal matrix. The function <code><a href="../reference/default.target.html">default.target()</a></code> offers some different approaches to selecting this. A good choice here is often the diagonal matrix times the reciprocal mean of the eigenvalues of the sample covariance as entries. See <code><a href="../reference/default.target.html">?default.target</a></code> for more choices.</p>
</div>
<div id="gaussian-graphical-modeling-and-post-processing" class="section level3">
<h3 class="hasAnchor">
<a href="#gaussian-graphical-modeling-and-post-processing" class="anchor"></a>Gaussian graphical modeling and post processing</h3>
<blockquote>
<h4 id="what-is-so-interesting-with-the-precision-matrix-anyway-im-always-interested-in-correlations-and-thus-the-correlation-matrix.">What is so interesting with the precision matrix anyway? I’m always interested in correlations and thus the correlation matrix.</h4>
</blockquote>
<p>As you may know, correlation does not imply causation. Nor does covariance imply causation. However, precision matrix provides stronger hints at causation. A relatively simple transformation of <span class="math inline">\(P\)</span> maps it to partial correlations—much like how the sample covariance <span class="math inline">\(S\)</span> easily maps to the correlation matrix. More precisely, the <span class="math inline">\(ij\)</span>th partial correlation is given by</p>
<p><span class="math display">\[
\rho_{ij|\text{all others}} = \frac{- p_{ij}}{\sqrt{p_{ii}p_{jj}}}
\]</span></p>
<p>where <span class="math inline">\(p_{ij}\)</span> is the <span class="math inline">\(ij\)</span>th entry of <span class="math inline">\(P\)</span>.</p>
<p>Partial correlations measure the linear association between two random variables whilst removing the effect of other random variables; in this case, it is all the remaining variables. This somewhat absolves the issue in “regular” correlations where are often correlated but only indirectly; either by sort of ‘transitivity of correlations’ (which does not hold generally and is not<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> so<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> simple<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>) or by common underlying variables.</p>
<blockquote>
<h4 id="ok-but-what-is-graphical-about-the-graphical-ridge-estimate">OK, but what is <strong>graphical</strong> about the graphical ridge estimate?</h4>
</blockquote>
<p>In a multivariate normal model, <span class="math inline">\(p_{ij} = p_{ji} = 0\)</span> if and only if <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> are conditionally independent when condition on all other variables. I.e. <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> are conditionally independent given all <span class="math inline">\(X_k\)</span> where <span class="math inline">\(k \neq i\)</span> and <span class="math inline">\(k \neq j\)</span> if and when the <span class="math inline">\(ij\)</span>th and <span class="math inline">\({ji}\)</span>th elements of <span class="math inline">\(P\)</span> are zero. In real world applications, this means that <span class="math inline">\(P\)</span> is often relatively sparse (lots of zeros). This also points to the close relationship between <span class="math inline">\(P\)</span> and the partial correlations.</p>
<p>The non-zero entries of the a symmetric PD matrix can them be interpreted the edges of a graph where nodes correspond to the variables.</p>
<blockquote>
<h4 id="graphical-ridge-estimation-why-not-graphical-lasso">Graphical ridge estimation? Why not graphical Lasso?</h4>
</blockquote>
<p>The graphical lasso (gLasso) is the L1-equivalent to graphical ridge. A nice feature of the L1 penalty automatically induces sparsity and thus also select the edges in the underlying graph. The L2 penalty of <em>rags2ridges</em> relies on an extra step that selects the edges after <span class="math inline">\(P\)</span> is estimated. While some may argue this as a drawback (typically due to a lack of perceived simplicity), it is often beneficial to separate the “variable selection” and estimation.</p>
<p>First, a separate post-hoc selection step allows for greater flexibility.</p>
<p>Secondly, when co-linearity is present the L1 penalty is “unstable” in the selection between the items. I.e. if 2 covariances are co-linear only one of them will typically be selected in a unpredictable way whereas the L2 will put equal weight on both and “average” their effect. Ultimately, this means that the L2 estimate is typically more stable than the L1.</p>
<p>At last point to mention here is also that the true underlying graph might not always be very sparse (or sparse at all).</p>
<blockquote>
<h4 id="how-do-i-select-the-edges-then">How do I select the edges then?</h4>
</blockquote>
<p>The <code><a href="../reference/sparsify.html">sparsify()</a></code> functions lets you select the non-zero entries of P corresponding to edges. It supports a handful different approaches ranging from simple thresholding to false discovery rate based selection.</p>
<p>After edge select <code><a href="../reference/GGMnetworkStats.html">GGMnetworkStats()</a></code> can be utilized to get summary statistics of the resulting graph topology.</p>
</div>
</div>
<div id="concluding-remarks" class="section level2">
<h2 class="hasAnchor">
<a href="#concluding-remarks" class="anchor"></a>Concluding remarks</h2>
<p>The <code><a href="../reference/fullMontyS.html">fullMontyS()</a></code> function is a convenience wrapper getting from the data through the penalized estimate to the corresponding conditional independence graph and topology summaries.</p>
<p>For a full introduction to the theoretical properties as well as more context to the problem, see <a href="https://www.sciencedirect.com/science/article/pii/S0167947316301141">van Wieringen &amp; Peeters (2016)</a>.</p>
<p><strong>rags2ridges</strong> also comes with functionality for <em>targeted</em> and <em>grouped</em> (or, <em>fused</em>) graphical ridge regression called the fused graphical ridge. See <a href="https://arxiv.org/abs/1509.07982">[2]</a> below. The functions in this <code>rags2ridges</code> module are generally post-fixed with <code>.fused</code>.</p>
<div id="references" class="section level3">
<h3 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h3>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0167947316301141">1.</a>. van Wieringen, W.N. and Peeters, C.F.W. <strong>(2016)</strong>. <a href="https://www.sciencedirect.com/science/article/pii/S0167947316301141"><em>Ridge Estimation of Inverse Covariance Matrices from High-Dimensional Data.</em></a> Computational Statistics &amp; Data Analysis, vol. 103: 284-303.</p>
<p><a href="https://arxiv.org/abs/1509.07982">2.</a> Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and van Wieringen, W.N. <strong>(2015)</strong>. <a href="https://arxiv.org/abs/1509.07982"><em>Targeted Fused Ridge Estimation of Inverse Covariance Matrices from Multiple High-Dimensional Data Classes.</em></a> arXiv:1509.07982 [stat.ME].</p>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Solution for the graphical ridge problem: <span class="math display">\[
P(\lambda) 
= \Bigg\{ \bigg[ \lambda I_{p\times p} + \frac{1}{4}(S - \lambda T)^{2} \bigg]^{1/2} + \frac{1}{2}(S -\lambda T) \Bigg\}^{-1}
\]</span><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="https://stats.stackexchange.com/questions/181376/is-correlation-transitive" class="uri">https://stats.stackexchange.com/questions/181376/is-correlation-transitive</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><a href="https://emilkirkegaard.dk/en/?p=5796" class="uri">https://emilkirkegaard.dk/en/?p=5796</a><a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/" class="uri">https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/</a><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Carel F.W. Peeters, Anders Ellern Bilgrau, Wessel N. van Wieringen.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
